{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocessing\n",
    "from data_preprocessing import*\n",
    "\n",
    "trainPath='C:/Aarohi DataFiles/Studies/MS/03 - Sem 3/CSCI 6364 - Machine Learning/Final Project/liar_dataset/train.tsv'\n",
    "testPath='C:/Aarohi DataFiles/Studies/MS/03 - Sem 3/CSCI 6364 - Machine Learning/Final Project/liar_dataset/test.tsv'\n",
    "validationPath='C:/Aarohi DataFiles/Studies/MS/03 - Sem 3/CSCI 6364 - Machine Learning/Final Project/liar_dataset/test.tsv'\n",
    "\n",
    "trainX,trainY,testX,testY,valX,valY=data_preprocess(trainPath,testPath,validationPath,pred=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "def swn_polarity(text):\n",
    "    \"\"\"\n",
    "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
    "    \"\"\"\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    " \n",
    " \n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    for raw_sentence in raw_sentences:\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    " \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    " \n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    " \n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    " \n",
    "    # judgment call ? Default to positive or negative\n",
    "    if not tokens_count:\n",
    "        return 0\n",
    " \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    if sentiment >= 0:\n",
    "        return 1\n",
    " \n",
    "    # negative sentiment\n",
    "    return 0\n",
    " \n",
    " \n",
    "sentiment=[]\n",
    "X = []  \n",
    "sentences = list(hate_comment[\"the_statement\"])\n",
    "for i in range(len(sentences)):\n",
    "    sen=preprocess_text(sentences[i])\n",
    "    if swn_polarity(sen)==1:\n",
    "        X.append(sen+\" positive\")\n",
    "        sentiment.append(1)\n",
    "    else:\n",
    "        X.append(sen+\" negative\")\n",
    "        sentiment.append(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment matrix with target liar labels\n",
    "#todo\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "def get_accuracy(review,sentiment,model,lr,show_coefficients=False):\n",
    "    review_model=model.fit_transform(review)\n",
    "    test_model=model.transform(testX)\n",
    "    #review_train,review_test,target_train,target_test=train_test_split(review_model,sentiment,test_size=0.2,random_state=0)\n",
    "    final_model=lr.fit(review_model,sentiment)\n",
    "    accuracy=final_model.score(test_model,testY)\n",
    "    print(\"Accuracy is \"+str(accuracy))\n",
    "    if show_coefficients:\n",
    "        df=pd.DataFrame({'Word':model.get_feature_names(),'Coefficient':final_model.coef_.tolist()[0]}).sort_values(['Coefficient','Word'],ascending=[0,1])\n",
    "        print(\"-----------------Top 25 positive words------------\")\n",
    "        print(df.head(10).to_string(index=False))\n",
    "        print(\"-----------------Top 25 negative words------------\")\n",
    "        print(df.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.7971864009378663\n",
      "-----------------Top 25 positive words------------\n",
      "       Word  Coefficient\n",
      "      today     1.601055\n",
      "    highest     1.559768\n",
      "     at the     1.235394\n",
      "      month     1.196493\n",
      "        got     1.123119\n",
      "     immigr     1.098391\n",
      "       wage     1.039887\n",
      " in america     1.005510\n",
      "       half     0.926486\n",
      "     of the     0.924621\n",
      "-----------------Top 25 negative words------------\n",
      "     Word  Coefficient\n",
      "       on    -0.851064\n",
      "     will    -0.902373\n",
      "  say the    -0.906583\n",
      "     know    -0.944415\n",
      "      war    -0.971157\n",
      "       if    -1.021704\n",
      "  medicar    -1.034999\n",
      " has been    -1.328472\n",
      "    abort    -1.378694\n",
      "     your    -1.418423\n"
     ]
    }
   ],
   "source": [
    "get_accuracy(trainX,trainY,TfidfVectorizer(ngram_range=(1,2),max_features=300),LogisticRegression(max_iter=300),show_coefficients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=300)\n",
      "MSE: 0.211\n",
      "Bias: 0.201\n",
      "Variance: 0.010\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "\n",
    "# estimate bias and variance\n",
    "model = TfidfVectorizer(ngram_range=(1,2),max_features=400)\n",
    "lr = LogisticRegression(max_iter=300)\n",
    "review_model=model.fit_transform(trainX)\n",
    "test_model = model.transform(testX)\n",
    "#print(review_model)\n",
    "#review_train,review_test,target_train,target_test=train_test_split(review_model,y,test_size=0.2,random_state=0)\n",
    "final_model=lr.fit(review_model,trainY)\n",
    "print(final_model)\n",
    "mse, bias, var = bias_variance_decomp(final_model, review_model, trainY, test_model, testY, loss='mse', num_rounds=200, random_seed=1)\n",
    "# summarize results\n",
    "print('MSE: %.3f' % mse)\n",
    "print('Bias: %.3f' % bias)\n",
    "print('Variance: %.3f' % var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====xgb boost classifier=====\n",
      "Accuracy is 0.7749120750293084\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "# Xgb boost Classifier to reduce bias\n",
    "\n",
    "print(\"=====xgb boost classifier=====\")\n",
    "\n",
    "model = TfidfVectorizer(ngram_range=(1,2),max_features=300,binary=True)\n",
    "review_model=model.fit_transform(X)\n",
    "test_model = model.transform(testX)\n",
    "\n",
    "#review_train,review_test,target_train,target_test=train_test_split(review_model,y,test_size=0.2,random_state=0)\n",
    "xgb = xgboost.XGBClassifier()\n",
    "final_model = xgb.fit(review_model, trainY)\n",
    "rf_results = xgb.predict(test_model)\n",
    "accuracy=final_model.score(test_model,testY)\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "              \"max_depth\"        : [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "              \"min_child_weight\" : [1, 3, 5, 7],\n",
    "              \"gamma\"            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n",
    "              \"colsample_bytree\" : [0.3, 0.4, 0.5 , 0.7]}\n",
    "\n",
    "randomXgb = RandomizedSearchCV(estimator=xgb,\n",
    "                               param_distributions=param_grid,\n",
    "                               n_iter=20,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=2,\n",
    "                               cv=3,\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of RandomizedSearchCV(cv=3,\n",
      "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                           colsample_bylevel=1,\n",
      "                                           colsample_bynode=1,\n",
      "                                           colsample_bytree=1, gamma=0,\n",
      "                                           gpu_id=-1, importance_type='gain',\n",
      "                                           interaction_constraints='',\n",
      "                                           learning_rate=0.300000012,\n",
      "                                           max_delta_step=0, max_depth=6,\n",
      "                                           min_child_weight=1, missing=nan,\n",
      "                                           monotone_constraints='()',\n",
      "                                           n_estimators=100, n_jobs=0,\n",
      "                                           num_pa...\n",
      "                                           reg_alpha=0, reg_lambda=1,\n",
      "                                           scale_pos_weight=1, subsample=1,\n",
      "                                           tree_method='exact',\n",
      "                                           validate_parameters=1,\n",
      "                                           verbosity=None),\n",
      "                   n_iter=20, n_jobs=-1,\n",
      "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
      "                                                             0.7],\n",
      "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
      "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
      "                                                          0.25, 0.3],\n",
      "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
      "                                                      15],\n",
      "                                        'min_child_weight': [1, 3, 5, 7]},\n",
      "                   random_state=42, verbose=2)>\n"
     ]
    }
   ],
   "source": [
    "finalXGB = randomXgb.estimator\n",
    "\n",
    "print(randomXgb.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 77.4912 %\n"
     ]
    }
   ],
   "source": [
    "# For liar dataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "y_pred_xgb = finalXGB.predict(test_model)\n",
    "\n",
    "print(\"Accuracy : {} %\".format(round(accuracy_score(testY, y_pred_xgb)*100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Gradient Boosting Classifier=====\n",
      "Accuracy is 0.794841735052755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "\n",
    "print(\"=====Gradient Boosting Classifier=====\")\n",
    "\n",
    "model = TfidfVectorizer(ngram_range=(1,2),max_features=300,binary=True)\n",
    "review_model=model.fit_transform(X)\n",
    "test_model = model.transform(testX)\n",
    "\n",
    "#review_train,review_test,target_train,target_test=train_test_split(review_model,y,test_size=0.2,random_state=0)\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=0)\n",
    "final_model = gbc.fit(review_model, trainY)\n",
    "gbc_results = gbc.predict(test_model)\n",
    "accuracy=final_model.score(test_model,testY)\n",
    "print(\"Accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=300)\n",
      "MSE: 0.211\n",
      "Bias: 0.201\n",
      "Variance: 0.010\n"
     ]
    }
   ],
   "source": [
    "# Bias and variance after boosting\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "\n",
    "# estimate bias and variance\n",
    "model = TfidfVectorizer(ngram_range=(1,2),max_features=400)\n",
    "lr = LogisticRegression(max_iter=300)\n",
    "review_model=model.fit_transform(trainX)\n",
    "test_model = model.transform(testX)\n",
    "#print(review_model)\n",
    "#review_train,review_test,target_train,target_test=train_test_split(review_model,y,test_size=0.2,random_state=0)\n",
    "final_model=lr.fit(review_model,trainY)\n",
    "print(final_model)\n",
    "mse, bias, var = bias_variance_decomp(final_model, review_model, trainY, test_model, testY, loss='mse', num_rounds=200, random_seed=1)\n",
    "# summarize results\n",
    "print('MSE: %.3f' % mse)\n",
    "print('Bias: %.3f' % bias)\n",
    "print('Variance: %.3f' % var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
